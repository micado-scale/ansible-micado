# MiCADO Worker Namespace ##
kind: Namespace
apiVersion: v1
metadata:
  name: micado-worker
  labels:
    name: micado-worker
---
# MiCADO Priority ##
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: micado-priority
  namespace: micado-worker
value: 900000000
globalDefault: false
description: "Priority class for MiCADO critical pods"
---
# Prometheus Service Account ##
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: micado-system
---
# Prometheus Cluster Role ##
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
  namespace: micado-system
rules:
- apiGroups: [""]
  resources:
  - nodes
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
# Prometheus Cluster Role Binding ##
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
  namespace: micado-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: micado-system
---
## MiCADO Dashboard ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: micado-dashboard
  namespace: micado-system
  labels:
    tier: frontend
spec:
  selector:
    matchLabels:
      name: micado-dashboard
      tier: frontend
  template:
    metadata:
      labels:
        name: micado-dashboard
        tier: frontend
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: micado-dashboard
        image: {{docker_images.dashboard}}
        env:
        - name: MICADO_FRONTEND_IP
          value: {{ ansible_host }}
        - name: MICADO_VERSION
          value: {{ micado_version }}
        - name: DISABLE_OPTIMIZER
          value: "{{ not enable_optimizer }}"
---
## Service: MiCADO Dashboard ##
apiVersion: v1
kind: Service
metadata:
  name: micado-dashboard
  namespace: micado-system
  labels:
    tier: frontend
spec:
  ports:
  - port: 4000
  selector:
    name: micado-dashboard
    tier: frontend
---
{% if enable_occopus %}
## Redis for Occopus ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: micado-system
  labels:
    tier: cloud-orchestration
spec:
  selector:
    matchLabels:
      name: redis
      tier: cloud-orchestration
  template:
    metadata:
      labels:
        name: redis
        tier: cloud-orchestration
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: occopus-redis
        image: {{docker_images.redis}}
        command:
        - redis-server
        args:
        - --appendonly
        - "yes"
        - --logfile
        - /tmp/redis-server.log
        volumeMounts:
        - name: redis-data
          mountPath: /data
        - name: redis-tmp
          mountPath: /tmp
      volumes:
      - name: redis-data
        hostPath:
          path: /var/lib/micado/redis/data
      - name: redis-tmp
        hostPath:
          path: /var/lib/micado/redis
---
## Service: Redis for Occopus ##
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: micado-system
  labels:
    tier: cloud-orchestration
spec:
  ports:
  - port: 6379
  selector:
    name: redis
    tier: cloud-orchestration
---
## Occopus ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: occopus
  namespace: micado-system
  labels:
    tier: cloud-orchestration
spec:
  selector:
    matchLabels:
      name: occopus
      tier: cloud-orchestration
  template:
    metadata:
      labels:
        name: occopus
        tier: cloud-orchestration
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: occopus
        image: {{docker_images.occopus}}
        args:
        - occopus-rest-service
        - --auth_data_path
        - /var/lib/micado/occopus/auth/auth_data.yaml
        - --host
        - 0.0.0.0
        - --parallelize
        env:
        - name: REDIS_NAME
          value: redis
        - name: LOG_DIR
          value: /var/log/occopus
        volumeMounts:
        - name: occopus-config
          mountPath: /root/.occopus/
        - name: occopus-data
          mountPath: /var/lib/micado/occopus/data
        - name: occopus-submitter
          mountPath: /var/lib/micado/occopus/submitter
        - name: occopus-auth
          mountPath: /var/lib/micado/occopus/auth
        - name: occopus-log
          mountPath: /var/log/occopus
      volumes:
      - name: occopus-config
        hostPath:
          path: /var/lib/micado/occopus/config
      - name: occopus-data
        hostPath:
          path: /var/lib/micado/occopus/data
      - name: occopus-submitter
        hostPath:
          path: /var/lib/micado/toscasubmitter/output
      - name: occopus-auth
        secret:
          secretName: cloud-credentials
      - name: occopus-log
        hostPath:
          path: /var/log/micado/occopus
---
## Service: Occopus ##
apiVersion: v1
kind: Service
metadata:
  name: occopus
  namespace: micado-system
  labels:
    tier: cloud-orchestration
spec:
  ports:
  - port: 5000
  selector:
    name: occopus
    tier: cloud-orchestration
---
{% endif %}
{% if enable_terraform %}
## Terraform ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: terraform
  namespace: micado-system
  labels:
    tier: cloud-orchestration
spec:
  selector:
    matchLabels:
      name: terraform
      tier: cloud-orchestration
  template:
    metadata:
      labels:
        name: terraform
        tier: cloud-orchestration
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: terraform
        image: {{docker_images.terraform}}
        tty: true
        command: ["/bin/sh"]
        env:
        - name: TF_INPUT
          value: "false"
        - name: TF_IN_AUTOMATION
          value: "true"
        volumeMounts:
        - name: submitter-out
          mountPath: /var/lib/micado/terraform/submitter
        - name: preprocess-files
          mountPath: /var/lib/micado/terraform/preprocess
        - name: wrapper-file
          mountPath: /usr/local/bin/terraform
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh", "-c", "apk -U add python3 py-requests py-jwt"]
      volumes:
      - name: submitter-out
        hostPath:
          path: /var/lib/micado/toscasubmitter/output
      - name: preprocess-files
        hostPath:
          path: /var/lib/micado/terraform/preprocess
      - name: wrapper-file
        hostPath:
          path: /var/lib/micado/terraform/wrapper/terraform
          type: File
---
{% endif %}
## Prometheus ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: micado-system
  labels:
    tier: monitoring
spec:
  selector:
    matchLabels:
      name: prometheus
      tier: monitoring
  template:
    metadata:
      labels:
        name: prometheus
        tier: monitoring
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      serviceAccount: prometheus
      containers:
      - name: prometheus
        image: {{docker_images.prometheus}}
        args:
        - --config.file=/etc/prometheus/prometheus.yml
        - --storage.tsdb.path=/prometheus
        - --web.console.libraries=/usr/share/prometheus/console_libraries
        - --web.console.templates=/usr/share/prometheus/consoles
        - --web.enable-lifecycle
        - --web.external-url=http://prometheus/prometheus/
        - --web.route-prefix=/prometheus
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
        - name: prometheus-data
          mountPath: /prometheus
      volumes:
      - name: prometheus-config
        hostPath:
          path: /var/lib/micado/prometheus/config
      - name: prometheus-data
        hostPath:
          path: /var/lib/micado/prometheus/data
---
## Service: Prometheus ##
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: micado-system
  labels:
    tier: monitoring
spec:
  ports:
  - port: 9090
  selector:
    name: prometheus
    tier: monitoring
---
## Alert Manager ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: micado-system
  labels:
    tier: monitoring
spec:
  selector:
    matchLabels:
      name: alertmanager
      tier: monitoring
  template:
    metadata:
      labels:
        name: alertmanager
        tier: monitoring
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: alertmanager
        image: {{docker_images.alertmanager}}
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager
        - name: alertmanager-data
          mountPath: /alertmanager
      volumes:
      - name: alertmanager-config
        hostPath:
          path: /var/lib/micado/alertmanager/config
      - name: alertmanager-data
        hostPath:
          path: /var/lib/micado/alertmanager/data
---
## Service: Alert Manager ##
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: micado-system
  labels:
    tier: monitoring
spec:
  ports:
  - port: 9093
  selector:
    name: alertmanager
    tier: monitoring
---
## PolicyKeeper ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: policykeeper
  namespace: micado-system
  labels:
    tier: scaling
spec:
  selector:
    matchLabels:
      name: policykeeper
      tier: scaling
  template:
    metadata:
      labels:
        name: policykeeper
        tier: scaling
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: policykeeper
        image: {{docker_images.policykeeper}}
        command:
        - /policykeeper/policy_keeper.py
        args:
        - --srv
        - --cfg
        - /config/policykeeper/policykeeper_config.yaml
        - --host
        - 0.0.0.0
        - --port
        - '12345'
        volumeMounts:
        - name: policykeeper-config
          mountPath: /config/policykeeper
        - name: pk-prom-config
          mountPath: /config/prometheus
        - name: policykeeper-log
          mountPath: /var/log/policykeeper
        - name: pk-kube-config
          mountPath: /root/.kube
        - name: docker-socket
          mountPath: /var/run/docker.sock
        - name: submitter-vol
          mountPath: /submitter
      volumes:
      - name: policykeeper-config
        hostPath:
          path: /var/lib/micado/policykeeper/config
      - name: pk-prom-config
        hostPath:
          path: /var/lib/micado/prometheus/config
      - name: policykeeper-log
        hostPath:
          path: /var/log/micado/policykeeper
      - name: pk-kube-config
        hostPath:
          path: /root/.kube
      - name: docker-socket
        hostPath:
          path: /var/run/docker.sock
      - name: submitter-vol
        hostPath:
          path: /var/lib/micado/toscasubmitter/output
---
## Service: PolicyKeeper ##
apiVersion: v1
kind: Service
metadata:
  name: policykeeper
  namespace: micado-system
  labels:
    tier: scaling
spec:
  ports:
  - port: 12345
  selector:
    name: policykeeper
    tier: scaling
---
{% if enable_optimizer %}
## Optimizer ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: optimizer
  namespace: micado-system
  labels:
    tier: optimizing
spec:
  selector:
    matchLabels:
      name: optimizer
      tier: optimizing
  template:
    metadata:
      labels:
        name: optimizer
        tier: optimizing
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: optimizer
        image: {{docker_images.optimizer}}
        args:
        - --cfg
        - /config/optimizer/optimizer_config.yaml
        - --host
        - 0.0.0.0
        - --port
        - '12346'
        volumeMounts:
        - name: optimizer-config
          mountPath: /config/optimizer
        - name: optimizer-log
          mountPath: /var/log/optimizer
      volumes:
      - name: optimizer-config
        hostPath:
          path: /var/lib/micado/optimizer/config
      - name: optimizer-log
        hostPath:
          path: /var/log/micado/optimizer
---
## Service: Optimizer ##
apiVersion: v1
kind: Service
metadata:
  name: optimizer
  namespace: micado-system
  labels:
    tier: optimizing
spec:
  ports:
  - port: 12346
  selector:
    name: optimizer
    tier: optimizing
---
{% endif %}
## Grafana ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: micado-system
  labels:
    tier: monitoring
spec:
  selector:
    matchLabels:
      name: grafana
      tier: monitoring
  template:
    metadata:
      labels:
        name: grafana
        tier: monitoring
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: grafana
        image: {{docker_images.grafana}}
        securityContext:
          runAsUser: 0
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: {{grafana_admin_pwd}}
        - name: GF_LOG_MODE
          value: console file
        volumeMounts:
        - name: grafana-config
          mountPath: /etc/grafana/grafana.ini
        - name: grafana-data
          mountPath: /var/lib/grafana
        - name: grafana-setup
          mountPath: /etc/grafana/provisioning
        - name: grafana-log
          mountPath: /var/log/grafana
      volumes:
      - name: grafana-config
        hostPath:
          path: /var/lib/micado/grafana/config/grafana.ini
      - name: grafana-data
        hostPath:
          path: /var/lib/micado/grafana/data
      - name: grafana-setup
        hostPath:
          path: /var/lib/micado/grafana/provisioning
      - name: grafana-log
        hostPath:
          path: /var/log/micado/grafana
---
## Service: Grafana ##
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: micado-system
  labels:
    tier: monitoring
spec:
  ports:
  - port: 3000
  selector:
    name: grafana
    tier: monitoring
---
## TOSCA Submitter ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: toscasubmitter
  namespace: micado-system
  labels:
    tier: orchestration
spec:
  selector:
    matchLabels:
      name: toscasubmitter
      tier: orchestration
  template:
    metadata:
      labels:
        name: toscasubmitter
        tier: orchestration
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: toscasubmitter
        image: {{docker_images.toscasubmitter}}
        args:
        - --bind
        - :5050
        volumeMounts:
        - name: docker-socket
          mountPath: /var/run/docker.sock
        - name: docker-binary
          mountPath: /usr/bin/docker
        - name: kube-config
          mountPath: /root/.kube
        - name: kube-binary
          mountPath: /usr/bin/kubectl
        - name: submitter-outputs
          mountPath: /var/lib/micado/submitter/files/output_configs
        - name: submitter-config
          mountPath: /var/lib/micado/submitter/system
        - name: submitter-preprocess-egi
          mountPath: /var/lib/micado/submitter/preprocess/egi
        - name: submitter-log
          mountPath: /var/log/submitter
{% if cloud_cred_file.stat.exists %}
        - name: submitter-auth-data
          mountPath: /var/lib/micado/submitter/auth
{% endif %}
{% if gce_cred_file.stat.exists %}
        - name: submitter-gce-auth
          mountPath: /var/lib/micado/submitter/gce-auth
{% endif %}
{% if oci_key_file.stat.exists %}
        - name: submitter-oci-auth
          mountPath: /var/lib/micado/submitter/oci-auth
{% endif %}
      volumes:
      - name: docker-socket
        hostPath:
          path: /var/run/docker.sock
      - name: docker-binary
        hostPath:
          path: /usr/bin/docker
      - name: kube-config
        hostPath:
          path: /root/.kube
      - name: kube-binary
        hostPath:
          path: /usr/bin/kubectl
      - name: submitter-outputs
        hostPath:
          path: /var/lib/micado/toscasubmitter/output
      - name: submitter-config
        hostPath:
          path: /var/lib/micado/toscasubmitter/system
      - name: submitter-preprocess-egi
        hostPath:
          path: /var/lib/micado/terraform/preprocess/egi
      - name: submitter-log
        hostPath:
          path: /var/log/micado/toscasubmitter
{% if cloud_cred_file.stat.exists %}
      - name: submitter-auth-data
        secret:
          secretName: cloud-credentials
{% endif %}
{% if gce_cred_file.stat.exists %}
      - name: submitter-gce-auth
        secret:
          secretName: gce-credentials
{% endif %}
{% if oci_key_file.stat.exists %}
      - name: submitter-oci-auth
        secret:
          secretName: oci-key
{% endif %}
---
## Service: TOSCA Submitter ##
apiVersion: v1
kind: Service
metadata:
  name: toscasubmitter
  namespace: micado-system
  labels:
    tier: orchestration
spec:
  ports:
  - port: 5050
  selector:
    name: toscasubmitter
    tier: orchestration
---
## Credential Manager ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: credman
  namespace: micado-system
  labels:
    tier: security
spec:
  selector:
    matchLabels:
      name: credman
      tier: security
  template:
    metadata:
      labels:
        name: credman
        tier: security
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: alertmanager
        image: {{docker_images.credential_manager}}
        env:
        - name: PROVISION_FILE
          value: /config/provisioning.csv
        - name: DATABASE_URL
          value: sqlite:////config/credential.db
        volumeMounts:
        - name: credman-config
          mountPath: /config
      volumes:
      - name: credman-config
        hostPath:
          path: /var/lib/micado/credman/config
---
## Service: Credential Manager ##
apiVersion: v1
kind: Service
metadata:
  name: credman
  namespace: micado-system
  labels:
    tier: security
spec:
  ports:
  - port: 5001
  selector:
    name: credman
    tier: security
---
{% if intel_cpu %}
## Image Intergrity Verifier ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: iivr
  namespace: micado-system
  labels:
    tier: security
spec:
  selector:
    matchLabels:
      name: iivr
      tier: security
  template:
    metadata:
      labels:
        name: iivr
        tier: security
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: iivr
        image: {{docker_images.iivr}}
        volumeMounts:
        - name: iivr-config
          mountPath: /config
      volumes:
      - name: iivr-config
        hostPath:
          path: /var/lib/micado/iivr/config
---
## Service: Image Intergrity Verifier ##
apiVersion: v1
kind: Service
metadata:
  name: iivr
  namespace: micado-system
  labels:
    tier: security
spec:
  ports:
  - port: 5000
  selector:
    name: iivr
    tier: security
---
{% endif %}
## Crypto Engine ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crypto-engine
  namespace: micado-system
  labels:
    tier: security
spec:
  selector:
    matchLabels:
      name: crypto-engine
      tier: security
  template:
    metadata:
      labels:
        name: crypto-engine
        tier: security
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: crypto-engine
        image: {{docker_images.crypto_engine}}
        volumeMounts:
        - name: crypto-engine-config
          mountPath: /config
      volumes:
      - name: crypto-engine-config
        hostPath:
          path: /var/lib/micado/crypto_engine/config
---
## Service: Crypto Engine ##
apiVersion: v1
kind: Service
metadata:
  name: crypto-engine
  namespace: micado-system
  labels:
    tier: security
spec:
  ports:
  - port: 5000
  selector:
    name: crypto-engine
    tier: security
---
## Vault ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vault
  namespace: micado-system
  labels:
    tier: security
spec:
  selector:
    matchLabels:
      name: vault
      tier: security
  template:
    metadata:
      labels:
        name: vault
        tier: security
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: vault
        image: {{docker_images.vault}}
        securityContext:
          capabilities:
            add: ["IPC_LOCK"]
        command:
        - vault
        args:
        - server
        - -config
        - /vault/config/vault.hcl
        volumeMounts:
        - name: vault-config
          mountPath: /vault/config
        - name: vault-storage
          mountPath: /vault/file
        - name: vault-log
          mountPath: /vault/logs
      volumes:
      - name: vault-config
        hostPath:
          path: /var/lib/micado/vault/config
      - name: vault-storage
        emptyDir: {}
      - name: vault-log
        hostPath:
          path: /var/log/micado/vault
---
## Service: Vault ##
apiVersion: v1
kind: Service
metadata:
  name: vault
  namespace: micado-system
  labels:
    tier: security
spec:
  ports:
  - name: tcp-8201
    port: 8201
  - name: tcp-8200
    port: 8200
  selector:
    name: vault
    tier: security
---
## Security Policy Manager ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: security-policy-manager
  namespace: micado-system
  labels:
    tier: security
spec:
  selector:
    matchLabels:
      name: security-policy-manager
      tier: security
  template:
    metadata:
      labels:
        name: security-policy-manager
        tier: security
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      initContainers:
      - name: init-wait-vault
        image: busybox:1.30
        command: ['sh', '-c', 'until nslookup vault; do echo waiting for vault; sleep 2; done;']
      containers:
      - name: security-policy-manager
        image: {{docker_images.securitypolicymanager}}
        readinessProbe:
          httpGet:
            path: /v1.0/nodecerts/ca
            port: 5003
          initialDelaySeconds: 5
          periodSeconds: 10
        volumeMounts:
        - name: kube-config
          mountPath: /root/.kube/config
      volumes:
      - name: kube-config
        hostPath:
          path: /root/.kube/config
---
## Service: Security Policy Manager ##
apiVersion: v1
kind: Service
metadata:
  name: security-policy-manager
  namespace: micado-system
  labels:
    tier: security
spec:
  clusterIP: 10.97.170.199
  ports:
  - port: 5003
  selector:
    name: security-policy-manager
    tier: security
---
## Zorp ##
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zorp
  namespace: micado-system
  labels:
    tier: security
spec:
  selector:
    matchLabels:
      name: zorp
      tier: security
  template:
    metadata:
      labels:
        name: zorp
        tier: security
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ''
      containers:
      - name: zorp
        image: {{docker_images.zorp}}
        ports:
        - hostPort: {{ web_listening_port | default('443') }}
          containerPort: 443
        volumeMounts:
        - name: zorp-config
          mountPath: /etc/zorp
        - name: zorp-scripts
          mountPath: /app
      volumes:
      - name: zorp-config
        hostPath:
          path: /var/lib/micado/zorp/config
      - name: zorp-scripts
        hostPath:
          path: /var/lib/micado/zorp/scripts
---
## Service: Zorp ##
apiVersion: v1
kind: Service
metadata:
  name: zorp
  namespace: micado-system
  labels:
    tier: security
spec:
  ports:
  - port: {{ web_listening_port | default('443') }}
    targetPort: 443
  selector:
    name: zorp
    tier: security
